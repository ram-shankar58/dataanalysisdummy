import pandas as pd
#!pip install seaborn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

from google.colab import drive
drive.mount('/content/drive')

# Load the file from Google Drive
file_path = '/content/drive/My Drive/Colab Notebooks/train.csv'
df = pd.read_csv(file_path)



data=pd.read_csv(file_path)
checkstats=data.describe()
print(checkstats)
data.head()
data.describe()
data.info()
data.isnull().sum()
import matplotlib.pyplot as plt
import seaborn as sns

'''plt.figure(figsize=(8, 6))
sns.histplot(data['state'], bins=30, kde=True)
plt.title('Histogram of Numeric Column')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(x='cat1', y='num2', data=data)
plt.title('Box Plot')
plt.xlabel('Category')
plt.ylabel('Numeric Column')
plt.xticks(rotation=90)
plt.show()
corr_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()
'''
''' other codes
import pandas as pd
import numpy as np

# Load your dataset (replace 'data.csv' with your actual file path)
data = pd.read_csv('data.csv')

# Define the numerical columns you want to analyze
numerical_columns = ['ph_no', 'cvv', 'credit_card_number', ...]  # Add all numerical column names

# Initialize a dictionary to store summary information
summary_report = {}

# Loop through each numerical column
for column in numerical_columns:
    column_data = data[column]
    
    # Calculate summary statistics
    summary_stats = {
        'Count': len(column_data),
        'Mean': column_data.mean(),
        'Std. Deviation': column_data.std(),
        'Min': column_data.min(),
        '25%': column_data.quantile(0.25),
        '50%': column_data.median(),
        '75%': column_data.quantile(0.75),
        'Max': column_data.max(),
    }
    
    # Define the number of bins for creating intervals
    num_bins = 20  # You can adjust this as needed
    
    # Create histogram bins
    hist, bin_edges = np.histogram(column_data, bins=num_bins)
    
    # Create labeled intervals
    intervals = [f'{bin_edges[i]:.2f} - {bin_edges[i+1]:.2f}' for i in range(num_bins)]
    
    # Add histogram data to the summary report
    summary_stats['Histogram'] = dict(zip(intervals, hist))
    
    # Add the summary statistics to the report for this column
    summary_report[column] = summary_stats

# Convert the summary report to a Pandas DataFrame for easy viewing
summary_df = pd.DataFrame(summary_report)

# Transpose the DataFrame for a more readable format
summary_df = summary_df.T

# Display the summary report
print(summary_df)

import pandas as pd
import numpy as np

# Create a DataFrame with your sample data
data = pd.DataFrame({
    'ID': [0, 1, 2],
    'ph_no': [3750960177, 5519318275, 3687518910],
    'cvv': [302, 914, 640],
    'credit_card_number': [349335196866487, 6570543684213065, 4190926541231336],
    'job': ['Psychotherapist', 'Medical illustrator', 'Camera operator'],
    'email': ['louisdavis@gmail.com', 'traciehayes@gmail.com', 'keithmacias@gmail.com'],
    'url': ['www.louisdavis.com', 'www.traciehayes.com', 'www.keithmacias.com'],
    'country': ['Bermuda', 'Namibia', 'Kazakhstan'],
    'name': ['Louis Davis', 'Tracie Hayes', 'Keith Macias'],
    '6aHwr': [0.9191865559974215, -1.67168979308964, 0.6835755247138586],
    'CbKM4': [-3.032131456576313, -0.169957509970028, 4.9642446744368485],
    'PwJxl': [2.3497706263348936, -3.5406434193995993, 2.2999354493900985],
    'rWVvg': [-2.786991920527277, -2.4970975175355594, -0.27298051636938814],
    '98Zw0': [3.196943555553763, 0.062396624335581574, 0.37792774794293726],
    '9buXh': [0.4776245887342052, -4.113783906513885, 2.904257283963671],
})

# Define the numerical columns you want to analyze
numerical_columns = ['ph_no', 'cvv', 'credit_card_number', '6aHwr', 'CbKM4', 'PwJxl', 'rWVvg', '98Zw0', '9buXh']

# Initialize a dictionary to store summary information
summary_report = {}

# Loop through each numerical column
for column in numerical_columns:
    column_data = data[column]
    
    # Calculate summary statistics
    summary_stats = {
        'Count': len(column_data),
        'Mean': column_data.mean(),
        'Std. Deviation': column_data.std(),
        'Min': column_data.min(),
        '25%': column_data.quantile(0.25),
        '50%': column_data.median(),
        '75%': column_data.quantile(0.75),
        'Max': column_data.max(),
    }
    
    # Define the number of bins for creating intervals
    num_bins = 20  # You can adjust this as needed
    
    # Create histogram bins
    hist, bin_edges = np.histogram(column_data, bins=num_bins)
    
    # Create labeled intervals
    intervals = [f'{bin_edges[i]:.2f} - {bin_edges[i+1]:.2f}' for i in range(num_bins)]
    
    # Add histogram data to the summary report
    summary_stats['Histogram'] = dict(zip(intervals, hist))
    
    # Add the summary statistics to the report for this column
    summary_report[column] = summary_stats

# Convert the summary report to a Pandas DataFrame for easy viewing
summary_df = pd.DataFrame(summary_report)

# Transpose the DataFrame for a more readable format
summary_df = summary_df.T

# Display the summary report
print(summary_df)



'''

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

# Load your dataset (replace 'data.csv' with your actual file path)
data = pd.read_csv('train.csv')

# Handle missing values
# Example: Impute missing numerical values with the mean
data.fillna(data.mean(), inplace=True)

# Encode categorical variables (you can choose between one-hot encoding and label encoding)
# Example: One-hot encoding for a categorical column 'country'
data = pd.get_dummies(data, columns=['country'], prefix='country')

# Scale numerical features
scaler = StandardScaler()
numerical_columns = ['ph_no', 'cvv', 'credit_card_number', '6aHwr', 'CbKM4', 'PwJxl', 'rWVvg', '98Zw0', '9buXh']
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# Split data into training and testing sets
X = data.drop(columns=['state'])  # Features
y = data['state']  # Target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a classification model (e.g., RandomForestClassifier)
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Evaluate the model
# Example: Calculate accuracy
accuracy = model.score(X_test, y_test)
print(f'Accuracy: {accuracy}')
